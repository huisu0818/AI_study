{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN (Fully Convolutional Network)\n",
    "Fully Convolutional Networks for Semantic Segmentation [ CVPR 2015  ·  Evan Shelhamer, Jonathan Long, Trevor Darrell ]\n",
    "\n",
    "https://paperswithcode.com/method/fcn\n",
    "\n",
    "https://kuklife.tistory.com/117\n",
    "\n",
    "## Abstract\n",
    "\n",
    "- 기존의 CNN을 이용한 Classification task에서 Segmentation task로 전환된 모델의 시초\n",
    "\n",
    "- 기존에는 fully connected layer를 통해 최종분류를 수행한다. 다만, 이럴 경우에 입력 이미지가 1차원 벡터로 변경되어 공간 정보가 제거된다.\n",
    "\n",
    "- Segmentation에서는 픽셀 단위로 classification을 수행해야 하기에 공간 정보를 유지하는 것이 중요하다.\n",
    "\n",
    "- FCN에서는 Fully convolutional layer를 통해 공간정보를 유지하도록 하여 segmentation task에 이용되었다. 또한 fully connected layer는 고정된 input size를 요구하는데, Fully convolutional layer에서는 input size가 달라져도 상관없다.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./FCN3.png\" alt=\"nn\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "- End-to-End:\n",
    "    - 입력에서 출력까지의 전체 모델이 단일 모델로 구성되어 있음을 의미한다.\n",
    "    - 입력데이터에서 최종 목표까지 모든 변환을 하나에 모델에서 처리\n",
    "    - 전처리 및 중간처리 등의 단계가 없어서 복잡성을 줄일 수 있다.\n",
    "    - 에시: 자율주행자동차에서 센서데이터를 입력으로 하여 최종데이터인 조향 및 가속도를 바로 출력하는 모델.\n",
    "\n",
    "- Pixels-to-Pixels:\n",
    "    - 입력이미지의 모든 픽셀을 출력이미지의 모든 픽셀로 매핑하는 모델을 의미.\n",
    "    - 입력이미지와 출력이미지의 모든 픽셀 관계를 학습.\n",
    "    - Segmentation, 영상변환, 이미지 복원 등에 사용될 수 있음.\n",
    "    - Segmentation에서는 이미지의 입력 픽셀을 해당 클래스에 대한 레이블로 매핑하는 모델을 의미.\n",
    "    - prediction feature map은 input과 동일한 size를 가져야 한다.\n",
    "    \n",
    "- 전체 patch를 사용하는 것이 빠른 수렴이 가능하여 patch를 sampling하기보다는 전체 이미지를 input으로 이용한다.(효율적인 학습)\n",
    "\n",
    "- Heatmap: class의 score와 spatial information을 담고 있다.\n",
    "    - convolution 층들을 통과하여 heatmap을 생성하는데, 이 heatmap의 개수는 class개수와 동일한 개수로 생성된다.\n",
    "    - 즉, heatmap은 각 픽셀별로 해당 class에 해당될 가능성에 대한 정보를 담게 된다.\n",
    "    - 이 heatmap의 size를 input size로 복원하여 모든 pixels에 대한 class를 예측한다. (dense prediction)\n",
    "    - 이 과정에서 Upsampling 기법이 적용된다.\n",
    "\n",
    "- Upsampling: Bilinear interpolation을 통해 Upsampling 과정에서 빈 픽셀을 적절히 채워준다.\n",
    "    - input 이미지와 동일한 feature map size를 얻게 하기 위함\n",
    "    - 손실된 spatial information과 data에 대한 충분한 대응이 되지는 않음\n",
    "\n",
    "- skip combining (skip connection): 손실된 information을 보완하기 위해 maxpooling된 feature map을 upsampling된 feature map과 결합\n",
    "\n",
    "## Detail\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./FCN.png\" alt=\"nn\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "- 위의 사진은 fully connected layer 대신에 fully convolutional layer로 구성된 모델을 나타낸다.\n",
    "\n",
    "- 이를 통해 spatial information을 최대한 보존한다.\n",
    "\n",
    "- 각 픽셀은 class에 대한 확률 벡터를 가지게 된다.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./FCN1.png\" alt=\"nn\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "- Fully connected layer를 사용했을 때에는 1차원 벡터로 전환되어 위치 정보 손실\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./FCN2.png\" alt=\"nn\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "- Fully convolutional layer를 사용하여 위치 정보 보존\n",
    "\n",
    "- $1\\times1$ convolution을 적용한 것\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./FCN4.png\" alt=\"nn\" width=\"500\">\n",
    "</div>\n",
    " \n",
    "- Upsampling:\n",
    "   - bilinear interpolation 기법과 deconvolution 기법이 적용됨(conv의 역 연산)\n",
    "   - Max Up pooling 등의 기법은 초기에 적용되지 않음\n",
    "   - upsample stride 8, 16, 32 -> feature map의 정보 하나를 32개로 upsampling 하겠다는 의미를 갖는다.\n",
    "   - bilinear interpolation 기법과 deconvolution 기법이 적용되었음에도 이미 줄어든 feature map을 통해 추정하는 것이기에 정보손실이 큰 것은 불가피.\n",
    "   - 이후 논문들은 이러한 기법의 불완전함을 지적하였다.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./FCN5.png\" alt=\"nn\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "- skip combining: upsample은 당연히 기존의 정보가 손실된 것을 보완할 수 없기에 이전 layer의 정보를 같이 사용하는 방법을 취한다.\n",
    "\n",
    "- 이전 layer에서 max pooling에 의해 생성된 feature map과 upsampling을 통해 얻어진 feature map을 pixelwise summation을 하여 prediction map을 생성한다.\n",
    "\n",
    "## Bilinear Interpolation\n",
    "\n",
    "- Bilinear interpolation: Bilinear interpolation(양선형 보간)은 두 축에 따른 선형 보간을 결합하여 임의의 위치에서의 값을 추정하는 보간 기법 중 하나이다. 주로 이미지 처리에서 크기 조정 및 회전과 같은 변환 작업에 사용된다. Bilinear interpolation은 먼저 가장 가까운 네 개의 픽셀을 선택하여 각 픽셀의 가중 평균을 계산한다. 이러한 가중 평균은 각 픽셀과 대상 위치 간의 거리에 비례하여 할당된다. 이렇게 하면 원본 이미지의 픽셀 값들 사이의 부드러운 변화를 보존하면서 대상 위치에서의 값을 추정할 수 있다. 간단한 $2\\times2$ 그리드를 예를 들어보자. 먼저, 2차원 그리드에서 대상 위치의 좌표를 결정한다. 그런 다음 이 좌표를 기반으로 가장 가까운 네 개의 픽셀을 선택한다. 이 네 개의 픽셀의 값은 각각 거리에 따라 가중 평균이 계산된다. 이렇게 하면 대상 위치에서의 값이 추정된다.\n",
    "\n",
    "- Bilinear interpolation의 장점:\n",
    "   - 부드러운 이미지 변환을 제공하여 이미지의 품질을 향상시킨다.\n",
    "   - 고해상도 이미지를 생성하는 등의 크기 조정 작업에서 효과적으로 사용된다.\n",
    "   - 계산량이 적고 구현이 간단하여 많은 컴퓨터 비전 및 이미지 처리 응용 프로그램에서 널리 사용된다.\n",
    "\n",
    "## 1x1 convolution\n",
    "\n",
    "- 차원 축소(Dimension Reduction):\n",
    "    - $1\\times1$ 합성곱은 입력의 채널 수를 줄이는 데 사용된다. 이를 통해 채널 간의 상관 관계를 감소시키고 계산 비용을 줄일 수 있다. 이러한 차원 축소는 모델의 복잡성을 줄이고, 과적합을 방지하며, 계산 효율성을 향상시킬 수 있다.\n",
    "- 비선형성(Non-linearity):\n",
    "    - $1\\times1$ 컨볼루션은 비선형성을 도입할 수 있다. 활성화 함수(예: ReLU)를 적용하여 비선형 변환을 수행하므로, 신경망의 표현력을 향상시킬 수 있다.\n",
    "- 특성 맵 조정(Feature Map Adjustment):\n",
    "    - $1\\times1$ 합성곱을 사용하여 특성 맵의 크기를 조정할 수 있다. 이는 입력 특성 맵의 크기와 출력 특성 맵의 크기를 조절하여 모델의 복잡성을 조절하거나 다른 계층과의 통합을 용이하게 할 수 있다.\n",
    "- 다양한 연산과 결합:\n",
    "    - $1\\times1$ 합성곱은 다른 연산과 결합하여 다양한 네트워크 아키텍처를 구축하는 데 사용될 수 있다. 예를 들어, Inception 모듈에서 1x1 컨볼루션은 다양한 채널 수를 조절하여 네트워크의 특성을 향상시키는 데 사용된다.\n",
    "\n",
    "\n",
    "## Performance\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./FCN6.png\" alt=\"nn\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "- 최종결과는 upsample stride에 따라 위와 같이 생성된다.\n",
    "\n",
    "- Real-time inference를 충족할 만큼 빠르지 못하다.\n",
    "    - down sampling과 up sampling 과정에서 많은 parameter로 인해 계산량이 많다.\n",
    "\n",
    "- Global context information을 무시한다.\n",
    "\n",
    "- FCN은 입력 이미지를 여러 스케일의 특성 맵으로 다운샘플링하여 세그멘테이션을 수행한다. 이러한 다운샘플링 과정에서 고해상도의 공간적인 정보는 손실될 수 있다. 이는 FCN이 global context information을 무시한다고 말할 수 있는 이유 중 하나이다. 다운샘플링을 거치면서 고해상도의 공간적인 정보가 손실되면, 각 픽셀에 대한 지역적인 정보만을 고려하여 세그멘테이션을 수행하게 된다. 이는 해당 픽셀 주변의 작은 부분 이미지만을 고려하여 클래스를 예측하므로, 해당 픽셀과 관련된 global context(ex: 이미지의 전체적인 구조, 배경/전경 관계 등)을 고려하지 않는다는 것을 의미한다. 따라서 FCN은 전역 맥락 정보를 무시하고 지역적인 정보에만 의존하여 세그멘테이션을 수행하게 된다. 이는 특히 큰 객체가 작은 객체에 의해 잘못 분할되거나 배경과 객체를 구분하기 어려운 경우에 문제가 될 수 있다. 이러한 문제를 해결하기 위해 최근에는 global context information을 고려하는 다양한 네트워크 구조와 기술이 제안되고 있다. 이러한 접근 방식은 입력 이미지의 전역적인 구조와 맥락을 고려하여 세그멘테이션의 정확도를 향상시키는 데 도움이 된다.\n",
    "\n",
    "\n",
    "## END\n",
    "\n",
    "- Downsampling(conv & maxpooling): capture semantic & contextual information (특징 추출) \n",
    "    - pre-trained model을 이용하여 학습\n",
    "    - Fully Convolution\n",
    "    \n",
    "- Upsampling(deconv & skip connection): maintain spatial information (위치 정보 보존)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchsummary import summary as model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-15          [-1, 256, 28, 28]               0\n",
      "           Conv2d-16          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-17          [-1, 512, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-20          [-1, 512, 14, 14]               0\n",
      "           Conv2d-21          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-22          [-1, 512, 14, 14]               0\n",
      "           Conv2d-23          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-24          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-25            [-1, 512, 7, 7]               0\n",
      "  ConvTranspose2d-26          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-27          [-1, 512, 14, 14]               0\n",
      "      BatchNorm2d-28          [-1, 512, 14, 14]           1,024\n",
      "  ConvTranspose2d-29          [-1, 256, 28, 28]       1,179,904\n",
      "             ReLU-30          [-1, 256, 28, 28]               0\n",
      "      BatchNorm2d-31          [-1, 256, 28, 28]             512\n",
      "  ConvTranspose2d-32          [-1, 128, 56, 56]         295,040\n",
      "             ReLU-33          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-34          [-1, 128, 56, 56]             256\n",
      "  ConvTranspose2d-35         [-1, 64, 112, 112]          73,792\n",
      "             ReLU-36         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-37         [-1, 64, 112, 112]             128\n",
      "  ConvTranspose2d-38         [-1, 32, 224, 224]          18,464\n",
      "             ReLU-39         [-1, 32, 224, 224]               0\n",
      "      BatchNorm2d-40         [-1, 32, 224, 224]              64\n",
      "           Conv2d-41          [-1, 2, 224, 224]              66\n",
      "================================================================\n",
      "Total params: 13,334,050\n",
      "Trainable params: 13,334,050\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 270.46\n",
      "Params size (MB): 50.87\n",
      "Estimated Total Size (MB): 321.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class FCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, 2, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x1 = x\n",
    "        x = self.block2(x)\n",
    "        x2 = x\n",
    "        x = self.block3(x)\n",
    "        x3 = x\n",
    "        x = self.block4(x)\n",
    "        x4 = x\n",
    "        x = self.block5(x)\n",
    "        x5 = x\n",
    "        \n",
    "        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = score + x4                                # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = score + x3                                # element-wise add, size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = score + x2                                # element-wise add, size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = score + x1                                # element-wise add, size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "        \n",
    "        return score\n",
    "\n",
    "model = FCN()\n",
    "model_summary(model, (3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = {'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31))}\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.ranges = ranges['vgg16']\n",
    "        self.features = models.vgg16(weights=pretrained).features\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n",
    "                x = self.features[layer](x)\n",
    "            output[\"x%d\"%(idx+1)] = x\n",
    "        return output\n",
    "\n",
    "    \n",
    "class FCNs(nn.Module):\n",
    "    def __init__(self, pretrained_net, n_class):\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.pretrained_net = pretrained_net\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 512, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn1     = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn2     = nn.BatchNorm2d(256)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn3     = nn.BatchNorm2d(128)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn4     = nn.BatchNorm2d(64)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, dilation=1, output_padding=1)\n",
    "        self.bn5     = nn.BatchNorm2d(32)\n",
    "        self.classifier = nn.Conv2d(32, n_class, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.pretrained_net(x)\n",
    "        \n",
    "        x5 = output['x5']  # size=(N, 512, x.H/32, x.W/32)\n",
    "        x4 = output['x4']  # size=(N, 512, x.H/16, x.W/16)\n",
    "        x3 = output['x3']  # size=(N, 256, x.H/8,  x.W/8)\n",
    "        x2 = output['x2']  # size=(N, 128, x.H/4,  x.W/4)\n",
    "        x1 = output['x1']  # size=(N, 64, x.H/2,  x.W/2)\n",
    "\n",
    "        score = self.bn1(self.relu(self.deconv1(x5)))     # size=(N, 512, x.H/16, x.W/16)\n",
    "        score = score + x4                                # element-wise add, size=(N, 512, x.H/16, x.W/16)\n",
    "        score = self.bn2(self.relu(self.deconv2(score)))  # size=(N, 256, x.H/8, x.W/8)\n",
    "        score = score + x3                                # element-wise add, size=(N, 256, x.H/8, x.W/8)\n",
    "        score = self.bn3(self.relu(self.deconv3(score)))  # size=(N, 128, x.H/4, x.W/4)\n",
    "        score = score + x2                                # element-wise add, size=(N, 128, x.H/4, x.W/4)\n",
    "        score = self.bn4(self.relu(self.deconv4(score)))  # size=(N, 64, x.H/2, x.W/2)\n",
    "        score = score + x1                                # element-wise add, size=(N, 64, x.H/2, x.W/2)\n",
    "        score = self.bn5(self.relu(self.deconv5(score)))  # size=(N, 32, x.H, x.W)\n",
    "        score = self.classifier(score)                    # size=(N, n_class, x.H/1, x.W/1)\n",
    "    \n",
    "        return score  # size=(N, n_class, x.H/1, x.W/1)\n",
    "    \n",
    "vgg16 = VGGNet(pretrained=True)\n",
    "model = FCNs(vgg16, 2)\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
