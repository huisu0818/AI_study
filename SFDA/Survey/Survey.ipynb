{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFDA Survey (Source-free Domain Adaptation)\n",
    "\n",
    "A Comprehensive Survey on Source-free Domain Adaptation [ 2023, Zhiqi Yu, Jingjing Li, Zhekai Du, Lei Zhu, Heng Tao Shen ]\n",
    "\n",
    "논문링크: https://paperswithcode.com/paper/a-comprehensive-survey-on-source-free-domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ 1. Introduction ]\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDA와 SFDA의 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./Fig1.png\" alt=\"nn\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UDA: Source data, Target data, Source label에 접근 가능\n",
    "- SFDA: 오직 Target data에만 접근 가능 (Source에는 접근 불가)\n",
    "    - 정보 보호 및 저작권 문제로부터 자유로움\n",
    "    - 리소스가 제한된 장치에서도 학습 가능\n",
    "- MEMO: SFDA 연구는 여러 가지 기술적 접근 방식을 포함하고 있으며, 이들은 대체로 데이터 기반(data-based) 방법과 모델 기반(model-based) 방법으로 분류될 수 있습니다. 데이터 기반 방법은 주로 타겟 데이터의 재구성 및 증강을 통해 도메인 차이를 줄이는 반면, 모델 기반 방법은 주로 소스 모델의 지식을 유지하면서 타겟 도메인에 적응하는 방식을 사용합니다. 이러한 방법들 각각은 도메인 적응의 문제를 해결하는 데 있어 중요한 기여를 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFDA의 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./Fig2.png\" alt=\"nn\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./Table1.png\" alt=\"nn\" width=\"700\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ 2. Overview ]\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./Table2.png\" alt=\"nn\" width=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFDA의 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소스 도메인 $\\mathcal{D}^{s} = \\{ \\{ \\mathcal{X}^s, \\mathcal{P}(\\mathcal{X}^s), d^s \\}, \\mathbf{L}^s \\}$에서 학습된 모델 $\\mathcal{M}$이 주어진다.\n",
    "- Pre-training stage: 소스 모델은 지도학습으로 pre-train된다.\n",
    "- Adaptation stage: 소스 도메인 데이터는 접근 불가능하며, SFDA의 목적은 Source-trained 모델이 Target domain $\\Phi^t = \\{\\{\\mathcal{X}^t, \\mathcal{P}(\\mathcal{X}^t), d^t \\}\\}$에 적응하도록 하는 것이다.\n",
    "    - 이때, $\\mathcal{M}(\\mathcal{X}^t) = \\mathcal{P}(\\mathcal{Y}^t|\\mathcal{X}^t)$ 의 모델의 출력이 높은 정확도를 갖도록 하는 것을 목표로 한다.\n",
    "    - $\\mathcal{P}(\\mathcal{Y}^t|\\mathcal{X}^t)$는 target data $\\mathcal{X}^t$에 대한 예측 결과이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFDA의 setting\n",
    "```대부분의 SFDA는 Closed-set, Single-Source 셋팅에 초점을 맞춘다```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특징 공간의 동일성 여부\n",
    "    - 이질 도메인 적응(Heterogeneous DA): $\\mathcal{P}(\\mathcal{X}^s) \\neq \\mathcal{P}(\\mathcal{X}^t), d_s \\neq d_t$인 상태로, 분포 적응과 특징 공간 적응이 모두 필요한 상태\n",
    "    - 동질 도메인 적응(Homogeneous DA): $\\mathcal{P}(\\mathcal{X}^s) \\neq \\mathcal{P}(\\mathcal{X}^t), d_s = d_t$인 상태로 특징 공간이 동일한 상태로 분포 적응만 수행 (대부분의 SFDA)\n",
    "- 레이블 집합의 대응관계에 따른 분류\n",
    "    - Closed-set: $\\mathbf{L}^s = \\mathbf{L}^t$\n",
    "    - Partial-set: $\\mathbf{L}^s \\supset \\mathbf{L}^t$\n",
    "    - Open-Set: $\\mathbf{L}^s \\subset \\mathbf{L}^t$\n",
    "- 소스와 타겟 도메인 수에 따른 분류\n",
    "    - Single-Source: $\\mathbf{L}^s \\rightarrow \\mathbf{L}^t$\n",
    "    - Multi-Source: $\\{\\mathbf{L}_{1}^s, \\ldots, \\mathbf{L}_{n}^s\\} \\rightarrow \\mathbf{L}^t$\n",
    "    - Multi-Task: $\\mathbf{L}^s \\rightarrow \\{\\mathbf{L}_{1}^t, \\ldots, \\mathbf{L}_{n'}^t\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"./Fig3.png\" alt=\"nn\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ 3. Data-Based Method ]\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Domain-based Reconstruction: 모델에 내재된 소스 도메인 정보를 통해 소스 도메인 데이터를 모방하거나, 중간 도메인 혹은 다른 도메인을 재구성\n",
    "- Image-based information extraction: 레이블이 없는 타겟 도메인 데이터에서 잠재적인 데이터 구조나 클러스터링 정보를 탐색하여 타겟 데이터만으로 도메인 적응 작업을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Domain-based Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 3.1.1 Virtual Domain Generation ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{UDA_M} = \\mathcal{L}_{cls}(\\mathcal{C}(\\mathcal{F}(\\mathcal{X}^s)), \\mathcal{Y}^s)+\\mathcal{L}_{div}(\\mathcal{F}(\\mathcal{X}^s), \\mathcal{F}(\\mathcal{X}^t)) \\tag{1}\n",
    "$$\n",
    "\n",
    "- 이 함수는 UDA에서 모델의 소스 도메인에서 지도 학습을 수행함과 동시에 타겟 도메인의 분포를 직접 정렬하는 일반적 패러다임이다.\n",
    "- $\\mathcal{L}_{cls}$: 소스 도메인에서의 지도 학습 손실로, 주로 cross-entropy를 사용\n",
    "- $\\mathcal{L}_{div}$: 정렬에 사용되는 기준함수\n",
    "- 다만, SFDA는 $\\mathcal{X}^s$를 adaptation에 사용할 수 없다.\n",
    "- 따라서 VDG에서는 source data의 부재를 보충하기 위한 새로운 도메인을 구성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{SFDA} = \\mathcal{L}_{gen}(\\mathcal{C}(\\mathcal{F}(\\mathcal{X}^v)))+\\mathcal{L}_{div}(\\mathcal{F}(\\mathcal{X}^v), \\mathcal{F}(\\mathcal{X}^t)) \\tag{2}\n",
    "$$\n",
    "\n",
    "- $\\mathcal{X}_v$: 가상 도메인을 의미한다.\n",
    "- $\\mathcal{L}_{gen}$: 가상 도메인의 생성 손실을 나타낸다.\n",
    "- 모델은 소스 도메인에서 사전 학습되었고, 따라서 모델의 출력을 탐색하여 도메인 분포를 시뮬레이션한다.\n",
    "- 식 3과 비교하였을 때, SFDA에서는 도메인 간의 정렬뿐만 아니라 가상 도메인 생성의 품질도 고려해야 한다.\n",
    "- 가상 도메인이 생성되면 SFDA의 정렬 문제는 UDA에서의 정렬 문제와 유사해진다.\n",
    "- ex:\n",
    "    - ADDA가 VDA-DA에서 사용된다.\n",
    "    - MMD가 STDA에서 사용된다.\n",
    "- 가상 도메인은 adversarial generarion 혹은 Gaussian distribution에 기반하여 생성될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L(C, F, D,} G) = \\mathcal{L}_{adv}+\\mathcal{L}_{con} \\tag{3}\n",
    "$$\n",
    "    \n",
    "- GAN(Generative Adversarial Network)의 아이디어에서 나온 adversarial generation이다.\n",
    "- $\\mathcal{L}_{con}$: 도메인 간의 의미적 일관성을 유지하기 위한 손실함수이다.\n",
    "- $\\mathcal{L}_{adv}$: domain discriminator 간의 적대적 손실을 나타낸다.\n",
    "    - 이는 (4)의 방정식을 따라 유도된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{adv}(G) = \\mathbb{E}_{y,\\tilde{x}}[\\log \\mathcal{D}(1-G(y, \\tilde{z}))], \\\\\n",
    "\\mathcal{L}_{adv}(\\mathcal{D}) = \\mathbb{E}_{x_t \\verb|~| \\mathcal{X}_t}[\\log \\mathcal{D}(x_t)]+\\mathbb{E}_{y, \\tilde{z}}[\\log (1-\\mathcal{D}(G(y, \\tilde{z})))] \\tag{4}\n",
    "$$\n",
    "\n",
    "- $\\tilde{z}$: 노이즈 벡터를 나타낸다.\n",
    "- $G(y, \\tilde{z})$: 사전 정의된 레이블 $y$를 조건으로 하는 generator를 나타낸다.\n",
    "- 3C-GAN: generator에 의해 생성된 $x_v=G(y, \\tilde{z})$와 레이블 $y$ 사이의 의미적 유사성 강조\n",
    "- classifier: 성능 향상을 위해 deterministic constraint, weight constraint, clustering-based contraint 제안.\n",
    "- SDDA: domain discriminator 기반의 consistency loss 제안.\n",
    "- CPGA: contrastive loss 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{x}=G(\\tilde{z}),~~\\tilde{z} \\verb|~| \\mathcal{N}(0, 1) \\tag{5}\n",
    "$$\n",
    "    \n",
    "- VDG를 위해 Gaussian distribution을 기반으로 소스 도메인의 분포를 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "D^v(f_v)=\\sum_{k=1}^{K} \\pi_k\\mathcal{N}(f_v|\\mu_k, \\sigma^2I) \\tag{6}\n",
    "$$\n",
    "\n",
    "- 소스 도메인의 분포를 여러 가우시안 분포의 혼합으로 간주.\n",
    "- VDM-DA: 가상 도메인의 분포를 특징 공간에서 위의 가우시안 혼합 모델로 구성\n",
    "- $f_v$: 가상 특징\n",
    "- $\\pi_k$: 혼합 계수이며 $\\sum_{k=1}^{K} \\pi_k=1$이다.\n",
    "- 매개변수 추정을 위해 경험적으로 $K$를 범주의 수로 설정\n",
    "- 소스 분류기의 가중치가 암묵적으로 각 범주에 대한 prototypical information을 포함한다고 간주\n",
    "- 모델의 평균과 표준 편차를 소스 분류기에 기반하여 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{x}_{s, aug}=\\lambda\\tilde{x}_{s}^{i}+(1-\\lambda)\\tilde{x}_{s}^{j}, \\\\ \\tilde{y}_{s, aug}=\\lambda\\tilde{y}_{s}^{i}+(1-\\lambda)\\tilde{y}_{s}^{j} \\tag{7}\n",
    "$$\n",
    "\n",
    "- 가상 도메인을 위해 생성 모델을 사용하는 것은 비용이 많이 들 뿐만 아니라, 기본 데이터 패턴이 복잡할 때 도메인 일반화(domain generalization)를 잘 수행하기도 어렵다.\n",
    "- Non-generative 접근 시도: 타겟 도메인에서 신뢰할 수 있는 데이터를 직접 선택하여 가상 소스 도메인을 구성\n",
    "- 타겟 이미지를 소스 모델에 입력하여 예측 엔트로피가 높은 샘플을 소스 도메인 분포에 더 가까운 것으로 간주\n",
    "- 그러나 이 방법은 가상 소스 도메인에 대한 데이터 부족 문제를 갖는다.\n",
    "    - Du: 가상 소스 도메인을 구성하기 위해 선택할 수 있는 샘플의 수가 타겟 도메인의 $1\\over{10}$에 불과하다는 것을 발견\n",
    "- 이러한 문제를 해결하기 위해 Mixup을 이용하여 가상 소스 도메인 데이터를 확장\n",
    "- $\\lambda$: mixup계수\n",
    "- $\\tilde{x}_{s}^{i}$, $\\tilde{x}_{s}^{j}$: 가상 소스 도메인에서 예측 엔트로피에 의해 선택된 샘플\n",
    "- $\\tilde{y}_{s}^{i}$, $\\tilde{y}_{s}^{j}$: 해당 레이블\n",
    "- 증강된 소스 도메인: $D_{aug}^{s} = D^s\\cup \\tilde{D}^{aug}$, $\\tilde{D}^{aug}=\\{\\tilde{x}_{s, aug},\\tilde{y}_{s, aug}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 3.1.2 Intra-domain Adversarial Exploration ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 핵심 아이디어: 소스 도메인의 데이터를 사용할 수 없기 때문에 타겟 도메인 데이터 내부에서 분리된 두 그룹을 만들어 서로 대립시키는 학습을 통해 모델이 정렬되도록 한다.\n",
    "    - 소스 유사 데이터 (similar)\n",
    "    - 소스 비유사 데이터 (dissimilar)\n",
    "- 위의 두가지 기준으로 타겟 도메인을 분류하고, 소스 모델의 예측 값을 기준으로 두 그룹 간의 차이를 학습한다.\n",
    "- 도메인 간의 정렬이 아닌 도메인 내에서의 정렬에 초점을 맞춘다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 전통적 방식의 이해 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1** Source training:\n",
    "$$\\min_{\\theta_{\\mathcal{F}}, \\theta_{c_1}, \\theta_{c_2}} \\sum_{i=1}^{2} \\mathcal{L}_{\\text{cls}}\\left( C_i\\left( \\mathcal{F}\\left( \\mathcal{X}^s \\right) \\right), \\mathcal{Y}^s \\right)$$\n",
    "\n",
    "- $\\mathcal{F}_{cls}$는 소스 도메인에서 사용하는 cross-entropy 손실함수이다.\n",
    "- $\\theta_{\\mathcal{F}}$는 특징 추출기 파라미터, $\\theta_{c_1}, \\theta_{c_2}$은 각 분류기의 파라미터로 먼저 소스 도메인 데이터로 모델을 훈련한다.\n",
    "- 즉, 특징 추출기, 분류기에 대해 소스 도메인을 잘 구분하는 상태로 훈련한다.\n",
    "<br>\n",
    "\n",
    "**Step 2** Maximum classifier prediction discrepancy:\n",
    "$$\\min_{\\theta_{\\mathcal{C}_1}, \\theta_{\\mathcal{C}_2}}\\mathcal{L}_{cls}(\\mathcal{X}^s, \\mathcal{Y}^s)-\\mathcal{L}_{dis}(\\mathcal{Y}_{1}^{t}|\\mathcal{X}^t,\\ \\mathcal{Y}_{2}^{t}|\\mathcal{X}^t)$$\n",
    "\n",
    "- 첫번째 항은 step1과 동일한 cross-entropy 손실이다.\n",
    "- 두번째 항은 두 분류기 간의 discrepancy를 측정하는 손실함수이며 이 과정에서 분류기 간의 불일치를 최대화하려고 시도한다.\n",
    "- 즉, 두 분류기에 대해 소소 도메인을 잘 구분하는 상태로 훈련하고, 타겟 도메인의 불일치는 최대화한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Step 3** Minimize classifier prediction discrepancy:\n",
    "$$\\min_{\\theta_{\\mathcal{F}}}\\mathcal{L}_{dis}(\\mathcal{Y}_{1}^{t}|\\mathcal{X}^t, \\mathcal{Y}_{2}^{t}|\\mathcal{X}^t)$$\n",
    "\n",
    "- $\\theta_{\\mathcal{F}}$를 조정하여 두 분류기의 예측 차이를 최소화하는 방식으로 업데이트 한다.\n",
    "- 다시 특징 추출기에 대해서는 두 분류기 간의 discrepancy를 최소화한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 수정 방안 ]\n",
    "- 소스 데이터가 없기 때문에 소스 분류기 중 하나의 파라미터를 고정하고, 타겟 분류기를 타겟 도메인의 레이블 없는 데이터를 통해 학습시킨다.\n",
    "- 소스 분류기의 예측 값을 통해 타겟 도메인 데이터를 소스 유사 데이터와 비유사 데이터로 나누고, 두 그룹 간의 예측 차이를 최대화하거나 최소화하도록 학습이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\theta_{\\mathcal{c}_t}} \\sum_{x \\in \\mathcal{X}_h^t} \\mathcal{L}_{\\text{dis}}\\left( p^s(x), p^t(x) \\right) - \\sum_{x \\in \\mathcal{X}_l^t} \\mathcal{L}_{\\text{dis}}\\left( p^s(x), p^t(x) \\right) \\tag{8}\n",
    "$$\n",
    "\n",
    "- $\\mathcal{X}_h^t$ 는 소스와 유사한 타겟 데이터이다.\n",
    "- $\\mathcal{X}_l^t$ 는 소스 비유사 타겟 데이터이다.\n",
    "- 즉, target classifier가 소스와 유사한 타겟데이터에 대해서는 discrepancy를 최소화하고, 소스 비유사 타겟데이터에 대해서는 discrepancy를 최대화 하도록 학습한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\theta_{\\mathcal{F}}} \\sum_{x \\in \\mathcal{X}^t} \\mathcal{L}_{\\text{dec}}\\left( p^s(x), p^t(x) \\right) \\tag{9}\n",
    "$$\n",
    "\n",
    "- 특징 추출기에 대해서는 타겟 feature를 두 분류기의 decision boundaries에 속하도록 유도한다.\n",
    "- 즉, decision loss를 통해 feature extractor의 일반화 성능을 강화하는 것이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ $A^2$ Net 의 구조 ]\n",
    "$$\n",
    "\\min_{\\theta_{\\mathcal{C}_t}}-\\sum_{i=1}^{n_t}\\left(\\alpha_{i}^{s} \\log \\left(\\sum_{k=1}^{K}p_{(i)k}^{st}\\right) + \\alpha_{i}^{t} \\log \\left(\\sum_{k=K+1}^{2K}p_{(i)k}^{st}\\right)\\right), \\\\\n",
    "\\min_{\\theta_{\\mathcal{F}}}-\\sum_{i=1}^{n_t}\\left(\\alpha_{i}^{t} \\log \\left(\\sum_{k=1}^{K}p_{(i)k}^{st}\\right) + \\alpha_{i}^{s} \\log \\left(\\sum_{k=K+1}^{2K}p_{(i)k}^{st}\\right)\\right) \\tag{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ BAIT의 구조 ]\n",
    "$$\n",
    "\\min_{\\theta_{\\mathcal{C}_t}}\\sum_{x\\in{\\mathcal{X}_{h}^{t}}}\\mathcal{D}_{SKL}\\left(p^s(x),p^t(x)\\right)-\\sum_{x\\in{\\mathcal{X}_{l}^{t}}}\\mathcal{D}_{SKL}\\left(p^s(x),p^t(x)\\right), \\\\\n",
    "\\min_{\\theta_{\\mathcal{F}}}\\sum_{i=1}^{n_t}\\sum_{k=1}^{K}[-p_{i,k}^{s}\\log p_{i,k}^{t}-p_{i,k}^{t}\\log p_{i,k}^{s}] \\tag{11}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 3.1.3 perturbed Domain Supervision ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 방법은 소스 도메인과 타겟 도메인의 feature가 domain-invariant feature space에 기반을 둔다고 가정한다.\n",
    "- 즉 소스 도메인과 타겟 도메인은 domain-invariant feature + domain-biased factors에 의해 구성된다.\n",
    "- model이 domain-invariant feature를 학습하도록 지도하기 위해 교란 혹은 혼동의 관점에서 타겟도메인 데이터는 소스 데이터를 교란시키기에 용이하다. (ex: CEM)\n",
    "- 즉, 타겟 데이터를 통해 모델에 적절한 교란을 가할 수 있고, 이를 통해 domain-invariant feature를 학습시키는 것이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ SOAP, SMT의 전략 ]\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\overrightarrow{\\varepsilon}\\left( \\mathcal{D}^t, \\mathcal{D}^I \\right)_{UDA} &= \\overrightarrow{\\varepsilon}(\\mathcal{D}^t, \\mathcal{D}^s) + \\overrightarrow{\\varepsilon}(\\mathcal{D}^s, \\mathcal{D}^I) \\\\\n",
    "\\overrightarrow{\\varepsilon}\\left( \\mathcal{D}^t, \\mathcal{D}^I \\right)_{SFDA} &= \\overrightarrow{\\varepsilon}(\\mathcal{D}^{t+}, \\mathcal{D}^t)\n",
    "\\end{cases} \\tag{12}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ AAA의 adversarial perturbation 전략 ]\n",
    "\n",
    "$$\n",
    "\\tilde{x}_{t} = x_t + \\frac{G(x_t)}{\\|G(x_t)\\|_2}\\epsilon, \\\\\n",
    "\\max_{\\theta_G}\\frac{1}{n_t}\\sum_{i=1}^{n_t}\\mathcal{L}_{cls}\\left(\\mathcal{C}(\\mathcal{F}(\\tilde{x}_t)), \\hat{y}_{t}^{i}\\right) \\tag{13}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\max_{\\theta_{\\mathcal{F}}, \\theta_{\\mathcal{C}}}\\frac{1}{n_t}\\sum_{i=1}^{n_t}\\mathcal{L}_{cls}\\left(\\mathcal{C}(\\mathcal{F}(\\tilde{x}_t)), \\hat{y}_{t}^{i}\\right) \\tag{14}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Image-based Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지는 두 요소를 갖고 있다.\n",
    "    - 이미지 라벨과 관련한 정보를 담고 있는 요소\n",
    "    - 이미지 스타일과 관련한 정보를 담고 있는 요소 (domain-specific)\n",
    "\n",
    "- Neighborhood Clustering: 타겟 이미지의 내용 정보를 바탕으로 하며, 타겟 데이터 자체가 명확한 구조와 클러스터링을 가지고 있다는 관찰에 기반한다. 따라서, 이웃하는 노드들 간의 일관성을 유지하여 클래스 내 거리를 증가시키고, 클래스 간 거리를 감소시켜 특징 공간에서 분류의 난이도를 줄이도록 요구된다. 이는 모델의 결정 경계에서 잘못된 분류가 주로 발생하기 때문이다.\n",
    "\n",
    "- Image Style Translation: 이미지의 스타일 정보에서 출발하며, 타겟 이미지의 내용 정보를 변경하지 않고 소스 스타일로 변환한다. 이를 통해 타겟 이미지는 소스 분류기가 더 잘 인식할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 3.2.1 Neighborhood Clustering ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 타겟 도메인 데이터의 분포가 명시적으로 소스 모델의 분류기와 일치하지 않을지라도, 타겟 데이터의 내재된 구조가 이웃 관계에 내포되어 있다는 관찰에 기반한다.\n",
    "- 가정: 타겟 도메인의 각 범주에 해당하는 데이터 포인트들이 소스 모델의 경계 내에 위치한 이웃들이 있다.\n",
    "- 방법: 분류가 어려운 샘플들은 이숫 간의 일관성을 유지하여 소스 도메인 분포에 매핑이 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{LSC} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\log \\left[ p(x_i) \\cdot \\mathbb{B}_S \\left( \\mathcal{N}_k \\right) \\right] + \\sum_{c=1}^{C} \\text{KL} \\left( \\overline{p}_c \\parallel q_c \\right), \\\\\n",
    "\\mathcal{N}_{\\{1, \\ldots, K\\}} = \\left\\{ \\mathcal{B}_F^j \\mid top - K \\left( \\cos \\left( \\mathcal{F} \\left( x_i \\right), \\mathcal{B}_F^j \\right) \\right), \\forall \\mathcal{B}_F^j \\in \\mathcal{B}_F \\right\\}, \\\\\n",
    "\\overline{p} = \\frac{1}{n} \\sum_{i=1}^{n} p_c \\left( x_i \\right), \\text{ and } \\text{q}_{\\{c=1, \\ldots, C\\}} = \\frac{1}{C} \\tag{15}\n",
    "$$\n",
    "\n",
    "- 로컬 구조 클러스터링(local structure clustering)은 매니폴드 학습(manifold learning)의 원리에 기반하며, 데이터가 고차원 공간 내의 저차원 매니폴드 상에 위치해 있다고 가정한다. \n",
    "- 이웃 노드를 선택하여, 이웃 점들 간의 거리나 유사성을 측정함으로써 타겟 도메인에서 군집화를 수행한다.\n",
    "- $\\mathcal{N}_k$: $x_i$와 코사인 유사도를 기준으로 가장 유사한 상위 $K$개의 샘플을 나타낸다.\n",
    "- $\\mathbb{B}_S \\left( \\mathcal{N}_k \\right)$: 선택된 이웃 샘플의 예측 일관성을 나타낸다.\n",
    "- $\\overline{p}_c$: 범주 $c$에 대한 예측 확률이다.\n",
    "- $q_c$: 예측이 특정 몇개의 클래스에 집중되는 것을 방지하기 위한 사전 확률 분포이다.\n",
    "- 추가로, 상호 이웃 일관성(reciprocal neighborhood consistency)이라는 개념이 도입된다. \n",
    "    - 두 샘플이 서로의 가장 가까운 이웃일 때 상호 이웃으로 간주된다. \n",
    "    - 상호 이웃을 중심으로 클러스터링을 진행하면 군집화 과정이 더욱 견고해지며, 모델의 타겟 도메인 성능이 향상된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ NRC의 전략 ]\n",
    "$$\n",
    "\\mathcal{L}_{\\mathcal{N}} = -\\frac{1}{n_t}\\sum_{i}\\sum_{k\\in\\mathcal{N}_{K}^{i}}A_{ik}\\mathcal{B}_{S,k}^{T}p_i, \\\\\n",
    "\\mathcal{L}_{E} = -\\frac{1}{n_t}\\sum_{i}\\sum_{k\\in\\mathcal{N}_{K}^{i}}\\sum_{m\\in E_{M}^{k}}r\\mathcal{B}_{S, m}^{T}p_i \\tag{16}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ AaD의 전략 ]\n",
    "$$\n",
    "P\\left( \\mathcal{C}_i \\right) = \\prod_{j \\in \\mathcal{C}_i} p_{ij} = \\prod_{j \\in \\mathcal{C}_i} \\frac{e^{p_i^T p_j}}{\\sum_{k=1}^{N_t} e^{p_i^T p_k}}, \\\\\n",
    "P\\left( \\mathbb{B}_i \\right) = \\prod_{j \\in \\mathbb{B}_i} p_{ij} = \\prod_{j \\in \\mathbb{B}_i} \\frac{e^{p_i^T p_j}}{\\sum_{k=1}^{N_t} e^{p_i^T p_k}} \\tag{17}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 3.2.2 Image Style Translation ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 스타일 변환은 동일한 콘텐츠를 다른 스타일로 렌더링하는 것을 목표로 한다. \n",
    "- 이 방법에는 두 가지 주요 손실 함수가 있다. \n",
    "    - 내용 손실(content loss):변환 전후의 일관된 의미 정보를 보장하는 역할. \n",
    "    - 변환 손실(transfer loss): 여러 중간 계층의 특징 통계에 기반하여 스타일 변환을 수행\n",
    "- 기존 방법에서는 대부분의 특징 통계가 그램 행렬(Gram matrix)이나 인스턴스 정규화(instance normalization)의 형태로 고려\n",
    "    - 이는 소스 도메인 스타일 데이터를 필요로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{\\text{content}} = \\left\\| \\mathcal{F}^N \\left( \\tilde{x}_t \\right) - \\mathcal{F}^N \\left( x_t \\right) \\right\\|_2, \\\\\n",
    "\\mathcal{L}_{\\text{style}} = \\frac{1}{N} \\sum_{n=1}^{N} \\left\\| \\mu_{\\text{current}}^n - \\mu_{\\text{stored}}^n \\right\\|_2 + \\left\\| \\sigma_{\\text{current}}^n - \\sigma_{\\text{stored}}^n \\right\\|_2 \\tag{18}\n",
    "$$\n",
    "\n",
    "- Hou 등은 배치 정규화(Batch Normalization, BN) 레이어에 저장된 이미지 배치의 평균 $\\mu_{\\text{stored}}$와 분산 $\\sigma_{\\text{stored}}$을 소스 도메인의 스타일을 대표하는 요소로 사용하여, 소스 없이 이미지 스타일 변환을 제안.\n",
    "- $F_N$: 소스 분류기에서의 $N$ 계층의 특징 맵.\n",
    "- $\\tilde{x}_t$: 생성기를 통해 소스 스타일로 변환된 이미지."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ CPSS의 전략 ]\n",
    "$$\n",
    "\\mathcal{L}_{intra} = \\sigma \\left(\\tilde{F}_{i, j}\\right) \\left(\\frac{F_{i, j}-\\mu \\left(F_{i, j}\\right)}{\\sigma \\left(F_{i, j}\\right)}\\right) + \\mu \\left(\\tilde{F}_{i, j}\\right), \\\\\n",
    "\\mathcal{L}_{inter} = \\sigma \\left(\\tilde{F}_{k, i, j}\\right) \\left(\\frac{F_{k, i, j}-\\mu \\left(F_{k, i, j}\\right)}{\\sigma \\left(F_{k, i, j}\\right)}\\right) + \\mu \\left(\\tilde{F}_{k, i, j}\\right) \\tag{19}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F = \\begin{bmatrix}\n",
    "F_{1,1} & \\cdots & F_{1,n_w} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "F_{n_h,1} & \\cdots & F_{n_h,n_w}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ 4. Model-Based Methods ]\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 생성을 중심으로 하거나 데이터 속성을 탐구하는 데이터 기반 방법들과는 달리, 모델 기반 방법들은 모델을 여러 서브 모듈로 분리한 후, 도메인 적응을 위해 일부 모듈의 파라미터를 조정한다.\n",
    "- 이 범주에서 자기 학습(self-training)이 가장 주도적인 패러다임이며, SFDA 연구 전반에서 가장 많이 사용되는 방법이다. \n",
    "- 자기 학습 방법은 주로 보조 모듈을 사용하여 모델의 강인성을 향상시키고, 다른 방법들과 결합하여 소스 도메인 지식을 보존한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Self-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 4.1.1 Pseudo Labeling ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 감독 정보가 없는 상황에서 가장 직관적인 아이디어는 소스 모델의 예측에 기반하여 타겟 샘플에 레이블을 부여한 후, 이 의사 레이블에 기반하여 자기 지도 학습을 수행하는 것이다.\n",
    "- 프로토타입 생성(Prototype Generation): 높은 신뢰도의 샘플이나 모든 샘플의 특징에 기반하여 클래스 프로토타입을 생성하는 것이다.\n",
    "- 의사 레이블 할당(Pseudo-label Assignment): 다른 샘플과 클래스 프로토타입 간의 거리나 유사성에 따라 의사 레이블을 지정하는 것이다.\n",
    "- 의사 레이블 필터링(Pseudo-label Filtering): 레이블의 순도를 유지하기 위해 일부 잡음이 포함된 레이블을 필터링하는 과정이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 프로토타입 생성 ]\n",
    "$$\n",
    "c_k = \\frac{\\sum_{x_t \\in \\mathcal{X}_t} \\delta_k \\left( \\mathcal{F} \\circ \\mathcal{G}_t (x_t) \\right) \\mathcal{G}_t (x_t)}\n",
    "{\\sum_{x_t \\in \\mathcal{X}_t} \\delta_k \\left( \\mathcal{F}_t (x_t) \\right)} \\tag{20}\n",
    "$$\n",
    "\n",
    "- 자기 엔트로피(self-entropy)를 통해 클래스 프로토타입을 선택하는 것이다.\n",
    "- 즉, 자기 엔트로피가 일정 임계값보다 큰 각 클래스의 샘플을 선택하는 방식이다.\n",
    "- Deep cluster에 기반하여 각 클래스의 중심점을 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 의사 레이블 할당 ]\n",
    "\n",
    "$$\n",
    "y_{\\tilde{t}}=\\text{arg}\\min_{k}\\mathcal{L}_{pse}(\\mathcal{G}(x_t), p_k) \\tag{21}\n",
    "$$\n",
    "\n",
    "- 클래스 프로토타입이 생성된 후, 각 샘플의 의사 레이블은 특징 공간에서 유사성 또는 거리 기반으로 다양한 클래스 프로토타입과 비교하여 얻어진다.\n",
    "- 여기서 $L_{\\text{pse}}$ 는 샘플  $x_t$ 가 클래스 $k$ 의 프로토타입 $p_k$ 과 얼마나 가까운지를 측정하는 함수이다.\n",
    "- 즉, 가장 가까운 클래스 프로토타입에 해당 샘플의 레이블을 부여하는 방식이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_{\\tilde{t}} = \\text{arg}\\min_{k}\\mathcal{F}\\circ\\mathcal{G}(x_t) \\tag{22}\n",
    "$$\n",
    "\n",
    "- 만약 프로토타입이 생성되지 않은 경우, 수식은 위와 같이 단순화 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 의사 레이블 필터링 ]\n",
    "- 도메인 차이에 따른 데이터 분포 차이로 인해, 모델이 잘못된 레이블을 예측할 가능성도 존재한다. \n",
    "- 이를 방지하기 위해 의사 레이블 필터링 과정이 필요하다.\n",
    "- Kim 등은 하우스도르프 거리(Hausdorff distance)를 사용하여 잘못된 레이블을 필터링하는 메커니즘을 제안\n",
    "    - 샘플이 가장 가까운 클래스 프로토타입과 두 번째로 가까운 프로토타입 간의 거리를 비교하여, 더 작은 쪽에 할당하는 방식으로 신뢰도 높은 레이블만 남김"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 4.1.2 Entropy Minimizaion ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 엔트로피 최소화는 반지도 학습(semi-supervised learning)과 비지도 학습(unsupervised learning) 방법에서 널리 응용되었고, SFDA 설정에서 처음으로 도입되었다. \n",
    "- 이전의 비지도 도메인 적응 방법은 주로 소스와 타겟 도메인 간의 특징 분포를 맞추어 모델의 적응 능력을 향상시키려 함.\n",
    "- 그러나 SFDA에서는 소스 도메인 데이터를 사용할 수 없으므로, 모델이 이미 도메인에 적응된 상태에서 각 샘플의 출력이 결정적일 것이라고 가정하고, 이를 역으로 모델의 최적화를 유도하는 데 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{ent} = -\\mathbb{E}_{x_t\\in\\mathcal{X}_t}\\sum_{k=1}^{K}\\delta_k(p_t(x_t))\\log\\delta_k(p_t(x_t)) \\tag{23}\n",
    "$$\n",
    "\n",
    "- 여기서 $p_t$ 는 타겟 샘플 $x_t$ 에 대한 모델의 예측값을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{div} = \\sum_{k=1}^{K}p_k\\log p_k \\tag{24}\n",
    "$$\n",
    "\n",
    "- 엔트로피 최소화가 모든 예측이 특정 클래스에 편향되는 사소한 해를 유도할 수 있다. \n",
    "- 이를 방지하기 위해 배치 엔트로피 최대화 손실을 사용하여 예측의 다양성을 보장한다.\n",
    "- $p_k$: 배치 내에서 각 클래스의 출력값을 누적하여 얻어진 값.\n",
    "    - 배치 내에서 각 범주에 대한 샘플 수가 균형을 이루어야 하며, 균등한 분포가 최대 엔트로피 값을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{IM}=\\mathcal{L}_{ent}+\\mathcal{L}_{div} \\tag{25}\n",
    "$$\n",
    "\n",
    "- 최종 엔트로피 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 4.1.3 Contrastive Learning ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자기 지도 학습 방법 중 하나인 대조 학습(contrastive learning)은 비지도 학습에서 주로 사용된다.\n",
    "- 긍정 쌍(positive pairs)은 더 가까이 모으고, 부정 쌍(negative pairs)은 멀어지도록 한다.\n",
    "- 대조 학습의 핵심은 동일한 샘플에 대해 긍정 및 부정 샘플 쌍을 구성하는 것이다. \n",
    "- 메모리 뱅크 기반(memory-bank based), 인코더 기반(encoder based), 미니배치 기반(mini-batch based)의 세 가지 방식으로 나뉜다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{Info} = -\\sum_{v^{+}\\in V^{+}}\\log \\frac{\\text{exp}(u^{T}v^{+}/\\tau)}{\\text{exp}(u^{T}v^{+}/\\tau)+\\sum_{v^{-}\\in V^{-}}\\text{exp}(u^{T}v^{-}/\\tau)} \\tag{26}\n",
    "$$\n",
    "\n",
    "- 대조 학습에서 자주 사용되는 InfoNCE 손실 함수이다.\n",
    "- $V^+$ 는 긍정 쌍,  $V^-$ 는 부정 쌍을 나타낸다.\n",
    "- $u$ 는 특정 샘플의 표현이고,  $\\tau$는 온도 매개변수(temperature parameter)이다. (강도)\n",
    "- UDA에서 긍정 쌍과 부정 쌍의 구성은 보통 타겟 도메인 샘플을 키(key)로 사용하고 소스 도메인 샘플을 쿼리(query)로 한다. \n",
    "    - 그러나 소스 데이터를 사용할 수 없는 경우에는 어떻게 쿼리를 구성하여 소스 도메인을 대표할 수 있을지가 핵심 문제이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 소스 분류기의 가중치를 사용하는 방식 ]\n",
    "$$\n",
    "\\mathcal{L}^{1}_{\\text{cdc}} = -\\sum_{m=1}^{M} \\mathbb{I}_{\\hat{y}_u = m} \\log \\frac{\\exp\\left(u^T w_s^m / \\tau\\right)}{\\sum_{j=1}^{M} \\exp\\left(u^T w_s^j / \\tau\\right)} \\tag{27}\n",
    "$$\n",
    "\n",
    "- $W_s = [w_1^s, \\dots, w_M^s]$: 소스 분류기의 가중치이다.\n",
    "- $\\mathbb{I}_{\\hat{y}_u = m}$: 의사 레이블이 할당된 인덱스 행렬이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 현재 배치에서 긍정 쌍의 유사정을 증가시키는 방식 ]\n",
    "$$\n",
    "\\mathcal{L}_{cdc}^{2} = -\\log \\frac{\\text{exp}(s_{ij})}{\\sum_{v=1}^{b}\\mathbb{I}_{v\\neq i}|\\gamma_{iv}|\\text{exp}(s_{iv})} \\tag{28}\n",
    "$$\n",
    "\n",
    "- $s_{ij}$: $i$번째 샘플과 $j$번째 샘플 간의 유사성을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [ 메모리 뱅크를 사용하여 과거 모델 출력값을 보존하는 방식 ]\n",
    "$$\n",
    "\\mathcal{L}_{cdc}^{3} = -\\sum_{x_q\\in X_t}\\log \\frac{\\text{exp}(q^{T}k_{+}^{t-m}/\\tau)\\gamma_{+}^{t-m}}{\\sum_{i=0}^{N}\\text{exp}(q^{t}k_{i}^{t-m}/\\tau)\\gamma_{i}^{t-m}} \\tag{29}\n",
    "$$\n",
    "\n",
    "- $N$: 저장된 키의 수를 의미\n",
    "- $\\gamma$: 신뢰성 매개변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 컴퓨터 비전 작업에서, 물체 영역을 찾아내는 것은 도메인 적응에 중요한 역할을 한다. \n",
    "- 전통적인 CNN 모델은 주로 배경 정보와 같은 로컬한 도메인 특화 정보를 포착하는 경향이 있어, 우리가 실제로 관심을 가지는 물체에 집중하는 데 한계가 있디. \n",
    "- 이러한 문제를 해결하기 위해, Self-attention 메커니즘이 도입.\n",
    "    - 각 요소가 다른 모든 요소와의 관계를 학습하여 특정 관심 영역을 강조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{Attn_{self}}(Q, K, V) = softmax\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V \\tag{30}\n",
    "$$\n",
    "\n",
    "- 여기서  $Q, K, V$는 각각 쿼리(Query), 키(Key), 밸류(Value)를 나타낸다.\n",
    "- $d_k$ 는 $K$의 차원을 의미. \n",
    "- 이 방식은 모델이 배경 대신 중요한 물체에 집중하게 유도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}_{Attn_{cross}}(Q_s, K_t, V_t) = softmax\\left(\\frac{Q_{s}K_{t}^{T}}{\\sqrt{d_k}}\\right)V_t \\tag{31}\n",
    "$$\n",
    "\n",
    "- 도메인 간 차이를 줄이기 위한 다양한 방법으로 확장\n",
    "- $Q_s$ 는 소스 도메인에서 추출한 쿼리\n",
    "- $K_t$와 $V_t$ 는 타겟 도메인에서 추출한 키와 밸류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ 5. Comparision ]\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터셋: Office-31, office-Home, VisDA-C\n",
    "- 방법 분류:\n",
    "    - Domain-based Reconstruction (DR)\n",
    "    - Image-based Information Extraction (IIE)\n",
    "    - Self-Training (ST)\n",
    "    - Self-Attention (SA)\n",
    "- 결과:\n",
    "    - ST가 가장 주목받고 있는 방법론이다.\n",
    "    - DR의 경우 흔하게 사용된다.\n",
    "    - IIE에서는 Neighborhood clustering이 효과적이다.\n",
    "    - SA는 transformer-based SFDA에서 아주 유망하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 논문 최종 요약\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 논문은 Source-Free Domain Adaptation (SFDA), 즉 소스 없는 도메인 적응에 대한 포괄적인 연구 조사이다. \n",
    "- SFDA는 도메인 적응(DA)의 한 유형으로, 소스 도메인 데이터를 전혀 사용하지 않고도 타겟 도메인에서 모델을 적응시키는 방법론을 다룬다. \n",
    "- 특히 기존 도메인 적응 방법들이 소스 데이터를 필요로 하는 한계를 극복하고, 프라이버시나 데이터 보호 등의 문제를 해결하기 위해 등장한 SFDA의 발전과 다양한 응용 사례를 종합적으로 다룬다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DA와 SFDA의 등장 배경\n",
    "    - 실생활에서 소스 데이터에 접근할 수 없는 경우가 많다.\n",
    "    - 이러한 문제를 해결하기 위해 SFDA가 등장했으며, 소스 데이터 없이 타겟 데이터와 소스에서 미리 학습된 모델만을 사용해 타겟 도메인에 적응.\n",
    "- SFDA 방법 분류\n",
    "    - Data-based (데이터 기반 방법)\n",
    "        - Domain-based Reconstruction (도메인 기반 재구성)\n",
    "            - Virtual Domain Generation (가상 도메인 생성): 소스 도메인 데이터를 사용할 수 없는 상황에서 가상의 소스 도메인 데이터를 생성해 타겟 도메인과의 적응을 수행\n",
    "            - Intra-domain Adversarial Alignment (도메인 내 적대적 정렬): 타겟 도메인 내에서 데이터를 두 그룹으로 나누고, 이들 간의 적대적 학습을 통해 타겟 데이터의 도메인 특성을 맞춤\n",
    "            - Perturbed Domain Supervision (도메인 교란 기반 감독): 타겟 데이터에 적절한 교란을 가해 모델이 이를 극복하도록 학습시켜 도메인 간 차이를 줄이는 방법\n",
    "        - Image-based Information Extraction (이미지 기반 정보 추출)\n",
    "            - Neighborhood Clustering (이웃 군집화): 타겟 데이터의 이웃 관계를 기반으로 클러스터링을 수행하여, 타겟 데이터 내의 구조를 유지하며 학습\n",
    "            - mage Style Translation (이미지 스타일 변환): 타겟 이미지의 스타일을 소스 이미지 스타일로 변환하여 소스 분류기에 더 적합하게 만드는 방식\n",
    "    - Model-based (모델 기반 방법)\n",
    "        - Self-training (자기 학습)\n",
    "            - seudo Labeling (의사 레이블링): 타겟 데이터에 대해 모델이 예측한 결과를 바탕으로 가짜 레이블을 할당하고 이를 사용하여 모델을 학습하는 방법\n",
    "            - Entropy Minimization (엔트로피 최소화): 모델의 예측값이 명확해지도록 유도하여 타겟 도메인에서 더 나은 예측을 하도록 하는 방법\n",
    "            - Contrastive Learning (대조 학습): 타겟 도메인에서의 데이터 간 긍정 및 부정 쌍을 구성하여 서로 가까운 샘플은 더 가까이, 멀리 있는 샘플은 더 멀어지도록 학습하는 방법\n",
    "        - Self-attention (자기 주의 메커니즘): 각 요소 간의 상호작용을 학습하여 중요한 부분에 더 집중하도록 하여 타겟 도메인의 데이터를 보다 효과적으로 처리하는 방법"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
